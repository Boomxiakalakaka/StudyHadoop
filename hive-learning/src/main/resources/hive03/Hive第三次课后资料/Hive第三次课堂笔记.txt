好，我这边网路是ok的

有什么问题现在可以问一下

赫赫哈嘿 19:50:35
我们这个Hive学完能达到什么水平？
把直播讲的hive、hive函数、hive案例、及后边5次课的hive数仓案例，都掌握，去企业中完全可以胜任数仓岗位的工作
当然，有些东西是需要在工作中积累、扩展的

hadoop生态圈
	hadoop
	hive
	hbase
	辅助框架：数据采集框架flume、etl工具sqoop、调度框架azkaban
spark
flink

============
课前回顾：
分区表
分桶表
	为了提高采样的效率
	有时需要做join操作的时候，课件避免全表数据join，提高效率
导入数据
导出数据

本节课：hive的企业级调优
hive表的数据压缩
在选取压缩算法的时候，主要考虑三个方面：
1、压缩率
2、压缩的时间
3、压缩算法时候支持切分
切分：
一个文件，经过压过压缩算法压缩后，结果上传到hdfs，它也是分块存储；300m-》3块
然后，如果有个mr，以此压缩后的文件为输入文件的话，那么
看这个压缩算法时候支持切分
如果支持，那么，会生成3个map task
如果不支持，那么只会有一个map task（它将3个block块作为一个map task的输入）

set hive.exec.compress.intermediate=true;
hql语句，编译后生成mr呢，可能是有多个mr
mr1 ->mr1的结果（中间结果） mr2 -> 最终结果
hive.exec.compress.intermediate就是设置，将中间结果做压缩

雷宏扬 20:21:55
snappy压缩，由于不支持切分，这样会数据倾斜吧？
比如说一个表的文件大小300（snappy压缩后的），如果300是一个文件的话，那么只有一个map task
又想用snappy压缩，又想要比较好的并行度，可以如何？
可以将hive的表文件变成多个
128、128、44三个snappy压缩后的文件，=》3个map task

xxzx_5641682 20:24:12
hql设置是session级别的 
刚才将的命令行里设置属性的例子，它只在当前session生效
如果想要永久生效的话，可以设置.hiverc文件

文件存储格式：
hive表：
行式存储
	textfile
	sequencefile
	优点：
		如果查询的时候，要查询复合条件的某些行（一整行）的时候，效率会高一些
	劣势：
		如果数据中，存在大量的null的话，这些null也会占据存储空间
列式存储
	orc
	parquet
	优点：
		如果查询的时候，查询特定的列的话，效率高
		相同列，相同类型的数据存储在一起，压缩效率更高
		
	缺点：

2017-08-10 13:00:00     http://www.taobao.com/item/962967_14?ref=1_1_52_search.ctg_1 T82C9WBFB1N8EW14YF2E2GY8AC9K5M5P http://www.yihaodian.com/ctg/s2/c24566-%E5%B1%
B1%E6%A5%82%E5%88%B6%E5%93%81?ref=pms_15_78_258 222.78.246.228  134939954       156

create table log_text (
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE ;

create table log_orc(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS orc ;

歸壹 20:38:42
列式存储，可以取一行数据吗，所有属性会不会对应不上。（空不存储）
肯定可以

create table log_parquet(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS PARQUET ;  

胖虎ヾ 20:43:49
老师，可以说一下，各个存储、压缩的应用场景么 
存储格式：
	企业中一般有orc格式
压缩：
	一般使用snappy、lzo两种
	
小明同学 20:45:10
orc 只是存储方式是列存储嘛， hql查询啥的都和行存储一样嘛 
hql可以查询hive表（hive表有行式存储的、列式的）

create table log_orc_none(
track_time string,
url string,
session_id string,
referer string,
ip string,
end_user_id string,
city_id string
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
STORED AS orc 
tblproperties ("orc.compress"="NONE");

表数据的各个字段间的分隔符是多个分割符的话？
\t
 
create table t1 (id String, name string)
row format serde 'org.apache.hadoop.hive.contrib.serde2.MultiDelimitSerDe'
WITH SERDEPROPERTIES ("field.delim"="##");

create  table t2(id int, name string)
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe' 
WITH SERDEPROPERTIES ("input.regex" = "^(.*)\\#\\#(.*)$");

未来已来 20:53:14
可以通过正则表达式设计同时处理多种分隔符吗？比如同时处理 ‘,’ 和 '\t‘
肯定是不行的
表中的字段的分隔符，只能是一种

hive企业级调优
1、fetch抓取
hql中：
select * from stu;
select sid form stu;
查询可以不经过mr，就能讲结果返回

2、本地模式
Mr.Jiang 20:59:01
不经过MR，是经过什么计算？ 
本地运行
类似之前学习mr的时候，直接在idea中运行mr，这个mr就是 本地运行
运行模式：本地模式、伪分布式模式、分布式模式

胖虎ヾ 21:05:08
本地运行为什么速度这个快？ 

join
老版本：小表 join 大表
小表数据适合放到分布式缓存的话，也可以考虑map join

新版本：
小表 join 大表 和 大表 join 小表性能没区别

join操作的时候，如果是多表进行join的话，最好是分开写hql语句
a join b => join c

0       20111230000009  599cd26984f72ee68b2b6ebefccf6aed        安徽合肥365房产网    http://hf.house365.com/

10      20111230000010  285f88780dd0659f5fc8acc7cc4949f2        IQ数码  1       1    http://www.iqshuma.com/

use myhive;
create table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) 
row format delimited fields terminated by '\t';

create table nullidtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\t';

create table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\t';

load data local inpath '/kkb/install/hivedatas/hive_big_table/*' into table ori; 
load data local inpath '/kkb/install/hivedatas/hive_have_null_id/*' into table nullidtable;

INSERT OVERWRITE TABLE jointable
SELECT a.* FROM nullidtable a JOIN ori b 
ON a.id = b.id;

mr -> key相同的数据，进入到一个reduce中做处理；
如果a中有一些id=null，b中也有一些id=null
id=null的这些数据是不是key=null，=》 一个reduce中处理 处理的数据量比较大
数据的倾斜

如果id=null的数据，不是有效的数据的话，我们可以做过滤或清洗，将id=null的数据先过滤掉，然后再join操作，这时数据量变小，效率提升

INSERT OVERWRITE TABLE jointable
SELECT a.* FROM (SELECT * FROM nullidtable WHERE id IS NOT NULL ) a JOIN ori b ON a.id = b.id;

但是有时：join条件为null的数据，也是有效数据，这时该怎么办？
id=null的值，替换一下

set hive.exec.reducers.bytes.per.reducer=32123456;
set mapreduce.job.reduces=7;

INSERT OVERWRITE TABLE jointable
SELECT a.*
FROM nullidtable a
LEFT JOIN ori b 
ON CASE WHEN a.id IS NULL THEN 'hive' ELSE a.id END = b.id;
所有的id=null全部替换成hive
但是，是不key=null =》 key=hive => 数据量还是比较大，还是被同一个reduce处理
还是数据倾斜

case when a then b when c then d else e end
 CASE WHEN a.id IS NULL THEN 'hive' ELSE a.id END = b.id;

INSERT OVERWRITE TABLE jointable
SELECT a.*
FROM nullidtable a
LEFT JOIN ori b ON CASE WHEN a.id IS NULL THEN concat('hive', rand()) ELSE a.id END = b.id;

CASE WHEN a.id IS NULL THEN concat('hive', rand()) ELSE a.id END 
id=null -> hive0000111
hive0000112
变成了不同的key
环节数据倾斜

赫赫哈嘿 21:30:22
如果是数据量很大，要做笛卡尔积该怎么操作？ 
一定要避免笛卡尔乘积查询


 select  count(distinct ip )  from log_text;
 如果数据量小的话，可以写
 但是，如果数据量大的话，强烈反对用这种做法
 只对应一个reduce task
 
 select count(ip) from 
 (select ip from log_text group by ip) t;
 数据量大的话，性能比较好
 
 1000*1000=1000000
 一定要避免笛卡尔乘积查询
 
数据剪裁
就是说做表查询的时候，尽量先将要查询的数据量减小

select * 
select sid 性能更高

如果是分区表的话，只针对关心的分区的数据做查询
where time='202002'

SELECT a.id
FROM bigtable a
LEFT JOIN ori b ON a.id = b.id
WHERE b.id <= 10;

SELECT a.id
FROM ori a
LEFT JOIN bigtable b ON (a.id <= 10 AND a.id = b.id);

0: jdbc:hive2://node03:10000> select * from order_partition;
Error: Error while compiling statement: FAILED: SemanticException [Error 10041]: No partition predicate found for Alias "order_partition" Table "order_partition" (state=42000,code=10041)

select * from order_partition where month='2019-03'; 

0: jdbc:hive2://node03:10000> select * from order_partition where month='2019-03' order by order_price; 
Error: Error while compiling statement: FAILED: SemanticException 1:61 In strict mode, if ORDER BY is specified, LIMIT must also be specified. Error encountered near token 'order_price' (state=42000,code=40000)

select * from order_partition where month='2019-03' order by order_price limit 3; 
 
explain
+----------------------------------------------------+--+
|                      Explain                       |
+----------------------------------------------------+--+
| STAGE DEPENDENCIES:                                |
|   Stage-1 is a root stage                          |
|   Stage-0 depends on stages: Stage-1               |
|                                                    |
| STAGE PLANS:                                       |
|   Stage: Stage-1                                   |
|     Map Reduce                                     |
|       Map Operator Tree:                           |
|           TableScan                                |
|             alias: order_partition                 |
|             Statistics: Num rows: 1 Data size: 189 Basic stats: PARTIAL Column stats: NONE |
|             Select Operator                        |
|               expressions: order_number (type: string), order_price (type: double), order_time (type: string), '2019-03' (type: string) |
|               outputColumnNames: _col0, _col1, _col2, _col3 |
|               Statistics: Num rows: 1 Data size: 189 Basic stats: PARTIAL Column stats: NONE |
|               File Output Operator                 |
|                 compressed: true                   |
|                 Statistics: Num rows: 1 Data size: 189 Basic stats: COMPLETE Column stats: NONE |
|                 table:                             |
|                     input format: org.apache.hadoop.mapred.TextInputFormat |
|                     output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat |
|                     serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe |
|                                                    |
|   Stage: Stage-0                                   |
|     Fetch Operator                                 |
|       limit: -1                                    |
|       Processor Tree:                              |
|         ListSink                                   |
|                                                    |
+----------------------------------------------------+--+

https://tech.meituan.com/2014/02/12/hive-sql-to-mapreduce.html
explain

 是不是map数越多越好？
 肯定不是
 如果有大量的小文件 -》考虑合并小文件
 
 是不是保证每个map处理接近128m的文件块，就高枕无忧了？
 也不是；
小文件合并
CombineHiveInputFormat

根据 ==computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))==公式
调整split的大小-》进而调整map task的个数


哲程是大腿😘 22:11:06
动态分区需要开启非严格模式，代表动态分区操作危险么？ 
并非如此

PMKTEST 22:11:46
这些参数应该是代码里面指定的吧 
比如修改map task可数
可以在hive命令里边：set mapreduce.input.fileinputformat.split.maxsize=10485760;
如果是mr编程的话，也可以在代码里设置

赫赫哈嘿 22:11:58
之前groupby那块，hive对数据进行负载均衡，hive是怎么知道发生数据倾斜了呢？
hql语句转换成mr -》
肯定知道哪些key的数据多啊
如果判断出来某个key数据量多，会出发负载均衡


哲程是大腿😘 22:13:39
本地模式当输入数据量小于这个值时采用local  mr的方式，老师改的50000000，依据是什么？生产的时候怎么确定？
没什么依据
数值怎么制定，这个可以找个例子，做个基准测试（考虑当前集群各节点性能）
数据达到多少时，超过此数据量，集群运行合适；小于的话，本地运行合适

刘勇-大数据005期 22:14:23
企业里面一般设置参数时是设置成永久生效的还是session级别的 
如果，确定了某些属性需要改，而且是基本都使用的话，就写成永久的
否则就是临时
未来已来 22:15:49
这肯定根据实际情况来 

总结：
hive的存储格式
	行式存储
		textfile
		sequencefile
	列式存储
		orc 企业中用的多
		parquet
hive的压缩
	企业中常用的lzo、snappy
	
hive的调优

赫赫哈嘿 22:22:45
hive索引 
鸡肋，用的不多

林泽斌 22:23:33
设置永久生效的参数是在哪里设置？hive-site.xml？ 
.hiverc
在HIVE_HOME下创建一个.hiverc文件
比如要想永久设置reduce个数为3
那么在此文件中写上
set mapreduce.job.reduces=3;

进入beeline
[hadoop@node03 hive-1.1.0-cdh5.14.2]$ pwd
/kkb/install/hive-1.1.0-cdh5.14.2
[hadoop@node03 hive-1.1.0-cdh5.14.2]$ beeline -i .hiverc
那么在进入beeline前，会先执行.hiverc中的hive命令
连接到hiveserver2后，会看到reduce 个数已经设置为3
 set mapreduce.job.reduces;
 +--------------------------+--+
|           set            |
+--------------------------+--+
| mapreduce.job.reduces=3  |
+--------------------------+--+














 
 






















